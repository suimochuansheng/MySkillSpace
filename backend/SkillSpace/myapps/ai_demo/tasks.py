# backend/SkillSpace/myapps/ai_demo/tasks.py

from asgiref.sync import async_to_sync
from celery import shared_task
from channels.layers import get_channel_layer

from .model_loader import stream_generate_answer

# è·å– Channel Layer å®ä¾‹ï¼ˆç”¨äºå‘ WebSocket æ¨é€æ¶ˆæ¯ï¼‰
channel_layer = get_channel_layer()


@shared_task(name="myapps.ai_demo.tasks.qwen_chat_task_streaming", bind=True)
def qwen_chat_task_streaming(self, task_id, prompt, session_id=None, history=None):
    """
    AI æµå¼å¯¹è¯ä»»åŠ¡ï¼ˆé€šè¿‡ WebSocket æ¨é€ï¼‰

    å‚æ•°ï¼š
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ï¼ˆç”¨äº WebSocket Channel å‘½åï¼‰
        prompt: ç”¨æˆ·æé—®
        session_id: ä¼šè¯IDï¼ˆå¯é€‰ï¼Œç”¨äºä¿å­˜å†å²è®°å½•ï¼‰
        history: å†å²å¯¹è¯è®°å½•ï¼ˆå¯é€‰ï¼‰

    å·¥ä½œæµç¨‹ï¼š
        1. Celery Worker æ¥æ”¶ä»»åŠ¡
        2. è°ƒç”¨ AI æ¨¡å‹æµå¼ç”Ÿæˆ
        3. æ¯ç”Ÿæˆä¸€ä¸ª token å°±æ¨é€åˆ° Redis Channel
        4. WebSocket Consumer ç›‘å¬ Channel å¹¶è½¬å‘ç»™å‰ç«¯
    """
    print(f"ğŸ“¥ [Celery Task] å¼€å§‹æ‰§è¡Œæµå¼ä»»åŠ¡: task_id={task_id}")

    if history is None:
        history = []

    # Channel Group åç§°ï¼ˆä¸ Consumer ä¸­ä¿æŒä¸€è‡´ï¼‰
    channel_group_name = f"ai_{task_id}"

    try:
        # è°ƒç”¨æ¨¡å‹çš„æµå¼ç”Ÿæˆå™¨
        generator = stream_generate_answer(prompt, history=history)

        # éå†ç”Ÿæˆå™¨ï¼Œé€ token æ¨é€
        for chunk in generator:
            token = chunk["token"]
            chunk_type = chunk["type"]

            # é€šè¿‡ Channel Layer æ¨é€æ¶ˆæ¯åˆ° WebSocket Consumer
            async_to_sync(channel_layer.group_send)(
                channel_group_name,
                {
                    "type": "ai_message",  # å¯¹åº” Consumer çš„ ai_message æ–¹æ³•
                    "token": token,
                    "chunk_type": chunk_type,
                    "task_id": task_id,
                },
            )

            # å¦‚æœæ”¶åˆ°ç»“æŸä¿¡å·ï¼Œåœæ­¢æ¨é€
            if chunk_type == "finish":
                print(f"âœ… [Celery Task] ä»»åŠ¡å®Œæˆ: task_id={task_id}")
                break

        # (å¯é€‰) ä¿å­˜å®Œæ•´å¯¹è¯è®°å½•åˆ°æ•°æ®åº“
        # if session_id:
        #     from .models import ChatRecord
        #     ChatRecord.objects.create(...)

        return {"status": "success", "task_id": task_id}

    except Exception as e:
        print(f"âŒ [Celery Task] ä»»åŠ¡å¤±è´¥: {str(e)}")

        # å‘é€é”™è¯¯æ¶ˆæ¯åˆ°å‰ç«¯
        async_to_sync(channel_layer.group_send)(
            channel_group_name,
            {
                "type": "ai_message",
                "token": f"ç³»ç»Ÿé”™è¯¯: {str(e)}",
                "chunk_type": "error",
                "task_id": task_id,
            },
        )

        return {"status": "error", "error": str(e)}


# ä¿ç•™åŸæœ‰çš„éæµå¼ä»»åŠ¡ï¼ˆç”¨äºæ‰¹é‡å¤„ç†åœºæ™¯ï¼‰
@shared_task(name="myapps.ai_demo.tasks.qwen_chat_task", bind=True)
def qwen_chat_task(self, prompt, resume_id=None):
    """
    AI å¯¹è¯/åˆ†æä»»åŠ¡ï¼ˆéæµå¼ï¼Œè¿”å›æœ€ç»ˆç»“æœï¼‰
    é€‚ç”¨äºæ‰¹é‡å¤„ç†åœºæ™¯
    """
    print(f"ğŸ“¥ [Task] æ”¶åˆ° AI ä»»åŠ¡ï¼Œç®€å†ID: {resume_id}")

    try:
        # è°ƒç”¨æµå¼ç”Ÿæˆå™¨å¹¶æ‹¼æ¥å®Œæ•´ç»“æœ
        generator = stream_generate_answer(prompt, history=[])
        full_result = ""

        for chunk in generator:
            if chunk["type"] in ["answer", "thinking"]:
                full_result += chunk["token"]

        print(f"ğŸ“¤ [Task] æ¨ç†å®Œæˆï¼Œç»“æœé•¿åº¦: {len(full_result)}")

        return {"status": "success", "result": full_result}

    except Exception as e:
        print(f"âŒ [Task Error] {str(e)}")
        return {"status": "error", "error": str(e)}
