# backend/SkillSpace/myapps/ai_demo/ai_engine.py

import time

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


class QwenEngine:
    """
    Qwen (é€šä¹‰åƒé—®) æ¨¡å‹å¼•æ“ - å•ä¾‹æ¨¡å¼
    ç¡®ä¿ä¸€ä¸ª Celery Worker è¿›ç¨‹åªåŠ è½½ä¸€æ¬¡æ¨¡å‹åˆ°æ˜¾å­˜
    """

    _instance = None
    model = None
    tokenizer = None

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def __init__(self):
        print("ğŸš€ [GPU Worker] æ­£åœ¨åˆå§‹åŒ– AI å¼•æ“ï¼ŒåŠ è½½æ¨¡å‹ä¸­...")
        try:
            # è¿™é‡Œæ›¿æ¢ä¸ºæ‚¨æœ¬åœ°æ¨¡å‹çš„çœŸå®è·¯å¾„ï¼Œæˆ–è€… ModelScope/HuggingFace çš„æ¨¡å‹ID
            # ä¾‹å¦‚: "Qwen/Qwen2.5-1.5B-Instruct"
            model_path = "Qwen/Qwen2.5-1.5B-Instruct"

            # æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸ–¥ï¸  æ£€æµ‹åˆ°è¿è¡Œè®¾å¤‡: {device} (RTX 3080 åº”è¯¥æ˜¾ç¤º cuda)")

            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=True
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ° GPU
                trust_remote_code=True,
                torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦èŠ‚çœæ˜¾å­˜
            )
            print("âœ… [GPU Worker] æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ [GPU Worker] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å¼€å‘é˜¶æ®µä¸ºäº†ä¸æŠ¥é”™ï¼Œå¯ä»¥å…ˆ mock ä¸€ä¸ª
            self.model = "MockModel"

    def chat(self, prompt):
        """
        æ‰§è¡Œæ¨ç†
        """
        if self.model == "MockModel":
            time.sleep(2)
            return f"ã€æµ‹è¯•æ¨¡å¼ã€‘æ”¶åˆ°æç¤ºè¯ï¼š{prompt}ã€‚CUDAæœªæ­£ç¡®åŠ è½½ï¼Œè¿™æ˜¯æ¨¡æ‹Ÿè¿”å›ã€‚"

        # çœŸå®çš„æ¨ç†é€»è¾‘
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®€å†åˆ†æåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt},
        ]
        text = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to("cuda")

        generated_ids = self.model.generate(model_inputs.input_ids, max_new_tokens=512)
        generated_ids = [
            output_ids[len(input_ids) :]
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]

        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[
            0
        ]
        return response


# å…¨å±€é¢„åŠ è½½å®ä¾‹
# æ³¨æ„ï¼šPython çš„æ¨¡å—åŠ è½½æœºåˆ¶ä¿è¯äº†è¿™æ˜¯çº¿ç¨‹å®‰å…¨çš„
# ä½†ä¸ºäº†æ§åˆ¶åŠ è½½æ—¶æœºï¼Œæˆ‘ä»¬åœ¨ Task é‡Œè°ƒç”¨ get_instance() æ›´å¥½
