# ai_chat/model_loader.py
import os
import re
import traceback
from threading import Lock, Thread

from django.conf import settings  # å¼•å…¥ Django settings ä»¥è·å–åŸºå‡†è·¯å¾„

# æ¡ä»¶å¯¼å…¥ AI ä¾èµ–ï¼ˆä»…åœ¨å¯ç”¨æ—¶å¯¼å…¥ï¼‰
try:
    import torch
    from modelscope import snapshot_download
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        BitsAndBytesConfig,
        TextIteratorStreamer,
    )

    AI_AVAILABLE = True
except ImportError:
    # AI ä¾èµ–ä¸å¯ç”¨ï¼Œå®šä¹‰å ä½ç¬¦ä»¥é¿å…è¿è¡Œæ—¶é”™è¯¯
    torch = None
    snapshot_download = None
    AutoModelForCausalLM = None
    AutoTokenizer = None
    BitsAndBytesConfig = None
    TextIteratorStreamer = None
    AI_AVAILABLE = False

model_lock = Lock()

# --------------------------
# æ¨¡å‹é…ç½®
# --------------------------
# 1. ç¡®ä¿ç¼“å­˜ç›®å½•æ˜¯ç»å¯¹è·¯å¾„ï¼Œé¿å…ç›¸å¯¹è·¯å¾„å¸¦æ¥çš„æ··æ·†
BASE_DIR = settings.BASE_DIR
MODEL_CACHE_DIR = os.path.join(BASE_DIR, "qwen_model_cache")

MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"
DEVICE = "cuda:0" if (torch and torch.cuda.is_available()) else "cpu"
MAX_PROMPT_LENGTH = 2000

# å…¨å±€å˜é‡
model = None
tokenizer = None
model_loaded = False

# ç¯å¢ƒå˜é‡æ§åˆ¶
ENABLE_MODEL_LOADING = os.getenv("ENABLE_AI_MODEL", "true").lower() == "true"
USE_AI_API = os.getenv("USE_AI_API", "false").lower() == "true"  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨ API

# Flash Attention å¼€å…³ï¼ˆéœ€è¦å…ˆå®‰è£… flash-attnï¼‰
# å®‰è£…å‘½ä»¤ï¼špip install flash-attn --no-build-isolation
ENABLE_FLASH_ATTENTION = os.getenv("ENABLE_FLASH_ATTENTION", "false").lower() == "true"

print(f"AIå¼•æ“æ¨¡å¼ï¼š{'é˜¿é‡Œäº‘API' if USE_AI_API else 'æœ¬åœ°å¤§æ¨¡å‹'}")
print(f"AIæ¨¡å‹åŠ è½½å¼€å…³ï¼š{'å¯ç”¨' if ENABLE_MODEL_LOADING else 'ç¦ç”¨'}")
print(
    f"Flash Attention: {'å¯ç”¨' if ENABLE_FLASH_ATTENTION else 'ç¦ç”¨ï¼ˆå®‰è£…åå¯å¯ç”¨ï¼‰'}"
)


def load_model_on_startup():
    """
    åœ¨åº”ç”¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹

    æ³¨æ„ï¼šå¦‚æœä½¿ç”¨ API æ¨¡å¼ï¼ˆUSE_AI_API=Trueï¼‰ï¼Œåˆ™è·³è¿‡æœ¬åœ°æ¨¡å‹åŠ è½½

    ä¼˜åŒ–è¦ç‚¹ï¼š
    1. è·³è¿‡è”ç½‘éªŒè¯ï¼ˆlocal_files_only=Trueï¼‰
    2. ç›´æ¥æŒ‡å®šè®¾å¤‡ï¼ˆdevice_map="cuda:0"ï¼‰
    3. å…³é—­åŒé‡é‡åŒ–ï¼ˆèŠ‚çœå¯åŠ¨æ—¶é—´ï¼‰
    4. å¯ç”¨ CUDA ä¼˜åŒ–
    5. æ”¯æŒ Flash Attention 2ï¼ˆéœ€è¦å®‰è£…ï¼‰
    """
    global model, tokenizer, model_loaded

    # =========================================================
    # ğŸš€ å¦‚æœä½¿ç”¨ API æ¨¡å¼ï¼Œè·³è¿‡æœ¬åœ°æ¨¡å‹åŠ è½½
    # =========================================================
    if USE_AI_API:
        print("[INFO] [ModelLoader] æ£€æµ‹åˆ° USE_AI_API=Trueï¼Œè·³è¿‡æœ¬åœ°æ¨¡å‹åŠ è½½")
        print("[INFO] å°†ä½¿ç”¨é˜¿é‡Œäº‘é€šä¹‰åƒé—® API")
        return

    # æ£€æŸ¥ AI ä¾èµ–æ˜¯å¦å¯ç”¨
    if not AI_AVAILABLE:
        print("[WARNING] AI dependencies not installed. Skipping model loading.")
        return

    if not ENABLE_MODEL_LOADING:
        return

    print("[INFO] [ModelLoader] å‡†å¤‡åŠ è½½æœ¬åœ° AI æ¨¡å‹...")
    print(f"[DIR] ç¼“å­˜ç›®å½•: {MODEL_CACHE_DIR}")

    try:
        # =========================================================
        # âš¡ ä¼˜åŒ– 1: CUDA æ€§èƒ½ä¼˜åŒ–ï¼ˆå¯åŠ¨æ—¶é…ç½®ï¼‰
        # =========================================================
        if torch.cuda.is_available():
            print("[CONFIG] å¯ç”¨ CUDA æ€§èƒ½ä¼˜åŒ–...")
            torch.backends.cudnn.benchmark = True  # cuDNN è‡ªåŠ¨è°ƒä¼˜
            torch.backends.cuda.matmul.allow_tf32 = True  # TF32 åŠ é€Ÿï¼ˆ3080 æ”¯æŒï¼‰
            torch.backends.cudnn.allow_tf32 = True
            torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜
            print(f"[OK] CUDA ä¼˜åŒ–å·²å¯ç”¨ (è®¾å¤‡: {torch.cuda.get_device_name(0)})")

        # =========================================================
        # âš¡ ä¼˜åŒ– 2: ç›´æ¥æŒ‡å®šæœ¬åœ°è·¯å¾„ï¼Œè·³è¿‡ snapshot_download
        # =========================================================
        local_model_path = rf"{MODEL_CACHE_DIR}\Qwen\Qwen2___5-7B-Instruct"

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if os.path.exists(local_model_path) and len(os.listdir(local_model_path)) > 0:
            print(f"[LOAD] æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹ï¼Œè·³è¿‡è”ç½‘æ ¡éªŒï¼Œç›´æ¥åŠ è½½: {local_model_path}")
            model_dir = local_model_path
        else:
            print("[WARNING] æœ¬åœ°è·¯å¾„æ— æ•ˆï¼Œå›é€€åˆ° ModelScope ä¸‹è½½/æ ¡éªŒæ¨¡å¼...")
            model_dir = snapshot_download(
                MODEL_NAME, cache_dir=MODEL_CACHE_DIR, revision="master"
            )

        # =========================================================
        # âš¡ ä¼˜åŒ– 3: åŠ è½½ Tokenizerï¼ˆè·³è¿‡è”ç½‘éªŒè¯ï¼‰
        # =========================================================
        print("[TOKENIZER] åŠ è½½ Tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_dir,
            trust_remote_code=True,
            padding_side="right",
            local_files_only=True,  # [OK] è·³è¿‡è”ç½‘éªŒè¯
            resume_download=False,  # [OK] ä¸å°è¯•ç»­ä¼ 
        )
        print("[OK] Tokenizer åŠ è½½å®Œæˆ")

        # =========================================================
        # âš¡ ä¼˜åŒ– 4: é‡åŒ–é…ç½®ï¼ˆå…³é—­åŒé‡é‡åŒ–ï¼‰
        # =========================================================
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,  # ä½¿ç”¨ float16
            bnb_4bit_use_double_quant=False,  # [OK] å…³é—­åŒé‡é‡åŒ–ï¼ˆå‡å°‘å¯åŠ¨æ—¶é—´ï¼‰
            bnb_4bit_quant_type="nf4",
        )

        # =========================================================
        # âš¡ ä¼˜åŒ– 5: åŠ è½½æ¨¡å‹ï¼ˆå¯ç”¨ Flash Attention 2ï¼‰
        # =========================================================
        print("[LOADING] æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°æ˜¾å­˜ (4-bit é‡åŒ–)...")

        # æ„å»ºæ¨¡å‹åŠ è½½å‚æ•°
        model_kwargs = {
            "device_map": "cuda:0",  # [OK] ç›´æ¥æŒ‡å®šè®¾å¤‡ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ
            "trust_remote_code": True,
            "quantization_config": bnb_config,
            "local_files_only": True,  # [OK] è·³è¿‡è”ç½‘éªŒè¯
            "resume_download": False,  # [OK] ä¸å°è¯•ç»­ä¼ 
        }

        # å¦‚æœå¯ç”¨ Flash Attentionï¼Œæ·»åŠ å‚æ•°
        if ENABLE_FLASH_ATTENTION:
            try:
                import flash_attn  # noqa: F401

                model_kwargs["attn_implementation"] = "flash_attention_2"
                print("[OPTIMIZE] Flash Attention 2 å·²å¯ç”¨")
            except ImportError:
                print("[WARNING] Flash Attention æœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡† Attention")
                print("[TIP] æç¤ºï¼špip install flash-attn --no-build-isolation")

        model = AutoModelForCausalLM.from_pretrained(model_dir, **model_kwargs).eval()

        model_loaded = True

        # æ˜¾ç¤ºåŠ è½½ä¿¡æ¯
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print(
            f"ğŸ“Š Attention å®ç°: {getattr(model.config, '_attn_implementation', 'standard')}"
        )

        # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            print(
                f"ğŸ’¾ æ˜¾å­˜å ç”¨: {allocated:.2f}GB (å·²åˆ†é…) / {reserved:.2f}GB (å·²é¢„ç•™)"
            )

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
        traceback.print_exc()
        model_loaded = False


def get_model():
    if not AI_AVAILABLE:
        raise RuntimeError(
            "AI dependencies not installed. Please install required packages."
        )
    if not ENABLE_MODEL_LOADING:
        raise RuntimeError("AIæ¨¡å‹åŠ è½½æœªå¯ç”¨")
    if not model_loaded or model is None:
        raise RuntimeError("æ¨¡å‹å°šæœªåŠ è½½å®Œæˆï¼Œè¯·ç¨åå†è¯•")
    return model, tokenizer


# --------------------------
# Prompt å’Œ ç”Ÿæˆé€»è¾‘ä¿æŒä¸å˜
# --------------------------
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚
è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼ŒåŠ¡å¿…ä¸¥æ ¼éµå®ˆæ ‡è®°æ ¼å¼ï¼š

<thinking>
åœ¨è¿™é‡Œå†™å‡ºä½ çš„è¯¦ç»†æ€è€ƒè¿‡ç¨‹ã€åˆ†ææ­¥éª¤
</thinking>

<answer>
åœ¨è¿™é‡Œç»™å‡ºæœ€ç»ˆçš„å®Œæ•´ç­”æ¡ˆ
</answer>

æ³¨æ„ï¼š
1. å¿…é¡»ä½¿ç”¨<thinking>å’Œ<answer>æ ‡è®°
2. thinkingæ ‡è®°å†…å†™æ€è€ƒè¿‡ç¨‹
3. answeræ ‡è®°å†…å†™æœ€ç»ˆç­”æ¡ˆ
"""


def stream_generate_answer(prompt: str, history: list = None):
    """
    æµå¼ç”Ÿæˆç­”æ¡ˆï¼ˆæ”¯æŒåŒå¼•æ“åˆ‡æ¢ï¼‰

    æ ¹æ®ç¯å¢ƒå˜é‡ USE_AI_API é€‰æ‹©ï¼š
    - True: ä½¿ç”¨é˜¿é‡Œäº‘ APIï¼ˆäº‘ç«¯éƒ¨ç½²ï¼‰
    - False: ä½¿ç”¨æœ¬åœ°å¤§æ¨¡å‹ï¼ˆæœ¬åœ°æ¼”ç¤ºï¼‰

    ä¼˜åŒ–è¦ç‚¹ï¼š
    1. ä¼˜åŒ–ç”Ÿæˆå‚æ•°ï¼ˆmax_new_tokens, top_p, top_kï¼‰
    2. å¯ç”¨ KV cache
    3. ä½¿ç”¨æ­£åˆ™é¢„ç¼–è¯‘
    """
    if history is None:
        history = []

    # =========================================================
    # âš¡ å¼•æ“é€‰æ‹©ï¼šæ ¹æ®ç¯å¢ƒå˜é‡å†³å®šä½¿ç”¨ API è¿˜æ˜¯æœ¬åœ°æ¨¡å‹
    # =========================================================
    if USE_AI_API:
        print("[INFO] ä½¿ç”¨é˜¿é‡Œäº‘ API å¼•æ“")
        from .api_engine import stream_generate_answer_api

        yield from stream_generate_answer_api(prompt, history)
        return

    # =========================================================
    # ä»¥ä¸‹æ˜¯æœ¬åœ°å¤§æ¨¡å‹å¼•æ“é€»è¾‘
    # =========================================================
    print("[INFO] ä½¿ç”¨æœ¬åœ°å¤§æ¨¡å‹å¼•æ“")

    try:
        loaded_model, loaded_tokenizer = get_model()
    except RuntimeError as e:
        yield {"token": f"ç³»ç»Ÿæç¤ºï¼š{str(e)}", "type": "answer"}
        return

    # =========================================================
    # âš¡ ä¼˜åŒ– 1: æ„å»ºæ¶ˆæ¯ï¼ˆé™åˆ¶å†å²é•¿åº¦ï¼‰
    # =========================================================
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    for msg in history[-10:]:  # é™åˆ¶æœ€è¿‘ 10 è½®å¯¹è¯
        role = "user" if msg.get("role") == "user" else "assistant"
        messages.append({"role": role, "content": msg.get("content")})
    messages.append({"role": "user", "content": prompt})

    text = loaded_tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

    inputs = loaded_tokenizer([text], return_tensors="pt").to(DEVICE)
    streamer = TextIteratorStreamer(
        loaded_tokenizer, skip_prompt=True, skip_special_tokens=True
    )

    # =========================================================
    # âš¡ ä¼˜åŒ– 2: ç”Ÿæˆå‚æ•°ä¼˜åŒ–ï¼ˆå…³é”®ï¼ï¼‰
    # =========================================================
    generation_kwargs = dict(
        inputs,
        streamer=streamer,
        max_new_tokens=2048,  # âœ… ä» 1024 é™åˆ° 512ï¼ˆå‡å°‘ç”Ÿæˆæ—¶é—´ï¼‰
        do_sample=True,  #
        temperature=0.7,
        top_p=0.8,  # âœ… ä» 0.9 é™åˆ° 0.8ï¼ˆå‡å°‘é‡‡æ ·èŒƒå›´ï¼‰
        top_k=40,  # âœ… æ·»åŠ  top_k é™åˆ¶
        repetition_penalty=1.1,  # âœ… é¿å…é‡å¤
        pad_token_id=loaded_tokenizer.eos_token_id,
        use_cache=True,  # âœ… å¯ç”¨ KV cache
    )

    # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
    thread = Thread(target=loaded_model.generate, kwargs=generation_kwargs)
    thread.start()

    # =========================================================
    # âš¡ ä¼˜åŒ– 3: æµå¼è¾“å‡ºä¼˜åŒ–ï¼ˆä½¿ç”¨XMLæ ‡è®°è§£æï¼‰
    # =========================================================
    # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
    thinking_start_pattern = re.compile(r"<thinking>")
    thinking_end_pattern = re.compile(r"</thinking>")
    answer_start_pattern = re.compile(r"<answer>")
    answer_end_pattern = re.compile(r"</answer>")

    current_type = "thinking"  # å½“å‰çŠ¶æ€ï¼šthinking/answer/none
    in_thinking = False
    in_answer = False
    full_content = ""
    buffer = ""  # ç¼“å†²åŒºï¼Œç”¨äºå¤„ç†æ ‡è®°

    for new_text in streamer:
        full_content += new_text
        buffer += new_text

        # æ£€æµ‹<thinking>å¼€å§‹æ ‡è®°
        if not in_thinking and thinking_start_pattern.search(buffer):
            in_thinking = True
            current_type = "thinking"
            # æ¸…é™¤æ ‡è®°æœ¬èº«ï¼Œä¸æ¨é€
            buffer = re.sub(r".*?<thinking>", "", buffer)
            continue

        # æ£€æµ‹</thinking>ç»“æŸæ ‡è®°
        if in_thinking and thinking_end_pattern.search(buffer):
            in_thinking = False
            current_type = "none"
            # æ¸…é™¤æ ‡è®°æœ¬èº«
            buffer = re.sub(r"</thinking>.*", "", buffer)
            if buffer:
                yield {"token": buffer, "type": "thinking"}
            buffer = ""
            continue

        # æ£€æµ‹<answer>å¼€å§‹æ ‡è®°
        if not in_answer and answer_start_pattern.search(buffer):
            in_answer = True
            current_type = "answer"
            # æ¸…é™¤æ ‡è®°æœ¬èº«
            buffer = re.sub(r".*?<answer>", "", buffer)
            continue

        # æ£€æµ‹</answer>ç»“æŸæ ‡è®°
        if in_answer and answer_end_pattern.search(buffer):
            in_answer = False
            current_type = "none"
            # æ¸…é™¤æ ‡è®°æœ¬èº«
            buffer = re.sub(r"</answer>.*", "", buffer)
            if buffer:
                yield {"token": buffer, "type": "answer"}
            buffer = ""
            continue

        # æ¨é€æ­£å¸¸å†…å®¹
        if current_type in ["thinking", "answer"] and buffer:
            # é¿å…æ ‡è®°è¢«æ‹†åˆ†ï¼ˆç­‰å¾…ä¸‹ä¸€ä¸ªtokenç¡®è®¤ï¼‰
            if not buffer.endswith("<") and not buffer.endswith("</"):
                yield {"token": buffer, "type": current_type}
                buffer = ""

    # å¤„ç†å‰©ä½™ç¼“å†²åŒº
    if buffer:
        yield {
            "token": buffer,
            "type": current_type if current_type != "none" else "answer",
        }

    # æµç»“æŸåå‘é€ finish ä¿¡å·
    yield {"token": "", "type": "finish"}
