# ai_chat/model_loader.py
import os
import traceback
from threading import Lock, Thread

import bitsandbytes as bnb
import torch
from modelscope import snapshot_download
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TextIteratorStreamer,
)

model_lock = Lock()

# --------------------------
# æ¨¡å‹é…ç½®
# --------------------------
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"
MODEL_CACHE_DIR = "./qwen_model_cache"
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
MAX_PROMPT_LENGTH = 2000  # ä¸APIé™åˆ¶ä¿æŒä¸€è‡´

# å…¨å±€å˜é‡ï¼šæ¨¡å‹å’Œtokenizer
model = None
tokenizer = None
model_loaded = False

# ç¯å¢ƒå˜é‡æ§åˆ¶æ˜¯å¦å¯ç”¨æ¨¡å‹åŠ è½½ï¼ˆé»˜è®¤å¯ç”¨çœŸå®æ¨¡å‹ï¼‰
ENABLE_MODEL_LOADING = os.getenv("ENABLE_AI_MODEL", "true").lower() == "true"

print(
    f"å½“å‰è¿è¡Œç¯å¢ƒï¼štorch={torch.__version__} | bitsandbytes={bnb.__version__} | CUDA={torch.version.cuda}"
)
print(f"AIæ¨¡å‹åŠ è½½å¼€å…³ï¼š{'å¯ç”¨ï¼ˆåå°çº¿ç¨‹åŠ è½½ï¼‰' if ENABLE_MODEL_LOADING else 'ç¦ç”¨'}")


def load_model_on_startup():
    """
    åœ¨åº”ç”¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹ï¼ˆç”±apps.pyè°ƒç”¨ï¼‰
    æ­¤å‡½æ•°åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼Œä¸ä¼šé˜»å¡Djangoå¯åŠ¨
    """
    global model, tokenizer, model_loaded

    if not ENABLE_MODEL_LOADING:
        print("æç¤ºï¼šAIæ¨¡å‹åŠ è½½å·²ç¦ç”¨ï¼Œå¦‚éœ€å¯ç”¨è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ENABLE_AI_MODEL=true")
        return

    print("ğŸ”„ å¼€å§‹åŠ è½½ AI æ¨¡å‹...")
    try:
        # ä¸‹è½½æ¨¡å‹
        model_dir = snapshot_download(
            MODEL_NAME, cache_dir=MODEL_CACHE_DIR, revision="master"
        )

        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_dir, trust_remote_code=True, padding_side="right", use_fast=False
        )

        # é‡åŒ–é…ç½®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float32,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )

        print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹ (FP32 Compute Mode)...")
        model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            device_map=DEVICE,
            trust_remote_code=True,
            quantization_config=bnb_config,
        ).eval()

        model_loaded = True
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼AIåŠŸèƒ½å·²å°±ç»ª")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
        traceback.print_exc()
        model_loaded = False


def get_model():
    """
    è·å–å·²åŠ è½½çš„æ¨¡å‹å’Œtokenizer
    å¦‚æœæ¨¡å‹æœªåŠ è½½æˆ–åŠ è½½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
    """
    if not ENABLE_MODEL_LOADING:
        raise RuntimeError("AIæ¨¡å‹åŠ è½½æœªå¯ç”¨ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ ENABLE_AI_MODEL=true")

    if not model_loaded or model is None or tokenizer is None:
        raise RuntimeError("æ¨¡å‹æœªæˆåŠŸåŠ è½½ï¼Œè¯·æ£€æŸ¥å¯åŠ¨æ—¥å¿—")

    return model, tokenizer


# --------------------------
# æ ¸å¿ƒä¿®æ”¹ç‚¹ 1: ä¿®æ”¹ç³»ç»Ÿæç¤ºè¯ï¼ŒåŠ å…¥æ€è€ƒè¿‡ç¨‹æŒ‡ä»¤
# --------------------------
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚
è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

æ€è€ƒï¼š<åœ¨è¿™é‡Œå†™å‡ºä½ çš„æ€è€ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬åˆ†æç”¨æˆ·æ„å›¾ã€é—®é¢˜æ‹†è§£ã€æ¨ç†æ­¥éª¤ç­‰>

ç­”æ¡ˆï¼š<åœ¨è¿™é‡Œç»™å‡ºæœ€ç»ˆçš„å®Œæ•´ç­”æ¡ˆ>

æ³¨æ„ï¼š
1. å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡º
2. æ€è€ƒéƒ¨åˆ†è¦è¯¦ç»†å±•ç¤ºä½ çš„æ¨ç†è¿‡ç¨‹
3. ç­”æ¡ˆéƒ¨åˆ†è¦æ¸…æ™°ã€å‡†ç¡®ã€å®Œæ•´
"""


def generate_answer(prompt: str, history: list = None) -> str:
    """
    ç”Ÿæˆå›ç­”
    :param prompt: å½“å‰ç”¨æˆ·é—®é¢˜
    :param history: å†å²å¯¹è¯åˆ—è¡¨ [{"role": "user", "content": "..."}, ...]
    """
    # è¾“å…¥éªŒè¯
    if not prompt.strip():
        return "è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ï¼"
    if len(prompt) > MAX_PROMPT_LENGTH:
        return f"è¾“å…¥è¿‡é•¿ï¼ˆå½“å‰{len(prompt)}å­—ï¼‰ï¼Œè¯·æ§åˆ¶åœ¨{MAX_PROMPT_LENGTH}å­—ä»¥å†…"

    # åˆå§‹åŒ–å†å²
    if history is None:
        history = []

    # è·å–å·²åŠ è½½çš„æ¨¡å‹
    try:
        loaded_model, loaded_tokenizer = get_model()
    except RuntimeError as e:
        return f"æ¨¡å‹æœªå¯ç”¨ï¼š{str(e)}"

    with model_lock:
        try:
            # æ„å»ºåŒ…å«å†å²è®°å½•çš„æ¶ˆæ¯åˆ—è¡¨
            messages = [{"role": "system", "content": SYSTEM_PROMPT}]

            # è¿½åŠ å†å²è®°å½• (é™åˆ¶æœ€è¿‘ 10 è½®ï¼Œé˜²æ­¢æ˜¾å­˜çˆ†æ»¡)
            for msg in history[-10:]:
                role = "user" if msg.get("role") == "user" else "assistant"
                messages.append({"role": role, "content": msg.get("content")})

            # è¿½åŠ å½“å‰é—®é¢˜
            messages.append({"role": "user", "content": prompt})

            text = loaded_tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            # ç¼–ç è¾“å…¥
            encoding = loaded_tokenizer([text], return_tensors="pt")
            input_ids = encoding.input_ids.to(DEVICE)
            attention_mask = encoding.attention_mask.to(DEVICE)

            # ç”Ÿæˆé…ç½®
            generated_ids = loaded_model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_new_tokens=1024,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.05,
                pad_token_id=loaded_tokenizer.eos_token_id,
            )

            # è§£ç è¾“å‡º
            generated_ids = [
                output_ids[len(input_ids) :]
                for input_ids, output_ids in zip(input_ids, generated_ids)
            ]
            answer = loaded_tokenizer.batch_decode(
                generated_ids, skip_special_tokens=True
            )[0]
            return answer

        except Exception as e:
            traceback.print_exc()
            return f"ç”Ÿæˆå‡ºé”™ï¼š{str(e)}"
        finally:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()


def stream_generate_answer(prompt: str, history: list = None):
    """
    æµå¼ç”Ÿæˆå›ç­”çš„ç”Ÿæˆå™¨å‡½æ•°
    Yields:
        dict: {"token": "ç‰‡æ®µ", "type": "thinking" | "answer"}
    """
    if history is None:
        history = []

    # è·å–å·²åŠ è½½çš„æ¨¡å‹
    try:
        loaded_model, loaded_tokenizer = get_model()
    except RuntimeError as e:
        yield {"token": f"æ¨¡å‹æœªå¯ç”¨ï¼š{str(e)}", "type": "answer"}
        return

    # 1. æ„å»º Prompt
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    for msg in history[-10:]:
        role = "user" if msg.get("role") == "user" else "assistant"
        messages.append({"role": role, "content": msg.get("content")})
    messages.append({"role": "user", "content": prompt})

    text = loaded_tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

    inputs = loaded_tokenizer([text], return_tensors="pt").to(DEVICE)

    # 2. åˆå§‹åŒ–æµå¼è¿­ä»£å™¨
    streamer = TextIteratorStreamer(
        loaded_tokenizer, skip_prompt=True, skip_special_tokens=True
    )

    # 3. é…ç½®ç”Ÿæˆå‚æ•°
    generation_kwargs = dict(
        inputs,
        streamer=streamer,
        max_new_tokens=1024,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=loaded_tokenizer.eos_token_id,
    )

    # 4. åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­å¯åŠ¨ç”Ÿæˆ
    thread = Thread(target=loaded_model.generate, kwargs=generation_kwargs)
    thread.start()

    # 5. ä¸»çº¿ç¨‹ä» streamer ä¸­è¯»å– tokenï¼Œå¹¶æ ¹æ®"æ€è€ƒï¼š"å’Œ"ç­”æ¡ˆï¼š"æ ‡è®°åˆ‡æ¢ç±»å‹
    current_type = "thinking"  # é»˜è®¤å…ˆè¾“å‡ºæ€è€ƒè¿‡ç¨‹
    full_content = ""

    for new_text in streamer:
        full_content += new_text

        # æ£€æµ‹æ˜¯å¦é‡åˆ°"ç­”æ¡ˆï¼š"æ ‡è®°ï¼Œåˆ‡æ¢åˆ°answerç±»å‹
        if "ç­”æ¡ˆï¼š" in full_content and current_type == "thinking":
            # æ‰¾åˆ°"ç­”æ¡ˆï¼š"çš„ä½ç½®
            answer_pos = full_content.find("ç­”æ¡ˆï¼š")

            # å¦‚æœå½“å‰tokenè·¨è¶Šäº†"ç­”æ¡ˆï¼š"åˆ†ç•Œçº¿ï¼Œéœ€è¦åˆ†æ®µå¤„ç†
            before_answer = full_content[: answer_pos + len("ç­”æ¡ˆï¼š")]
            current_len_before_token = len(full_content) - len(new_text)

            if current_len_before_token < answer_pos + len("ç­”æ¡ˆï¼š"):
                # å½“å‰tokenåŒ…å«äº†"ç­”æ¡ˆï¼š"æ ‡è®°
                # å°†"ç­”æ¡ˆï¼š"ä¹‹å‰çš„éƒ¨åˆ†ä½œä¸ºthinking
                thinking_part_len = (
                    answer_pos + len("ç­”æ¡ˆï¼š") - current_len_before_token
                )
                if thinking_part_len > 0 and thinking_part_len <= len(new_text):
                    thinking_part = new_text[:thinking_part_len]
                    answer_part = new_text[thinking_part_len:]

                    # å…ˆå‘é€thinkingéƒ¨åˆ†ï¼ˆåŒ…å«"ç­”æ¡ˆï¼š"æ ‡è®°ï¼‰
                    if thinking_part:
                        yield {"token": thinking_part, "type": "thinking"}

                    # åˆ‡æ¢ç±»å‹
                    current_type = "answer"

                    # å‘é€answeréƒ¨åˆ†
                    if answer_part:
                        yield {"token": answer_part, "type": "answer"}
                    continue

            # åˆ‡æ¢ç±»å‹ï¼ˆé€‚ç”¨äº"ç­”æ¡ˆï¼š"å·²ç»åœ¨ä¹‹å‰çš„tokenä¸­å®Œæ•´å‡ºç°çš„æƒ…å†µï¼‰
            current_type = "answer"

        # æ­£å¸¸å‘é€token
        yield {"token": new_text, "type": current_type}
